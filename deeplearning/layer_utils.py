from deeplearning.layers import *


def affine_relu_forward(x, w, b):
    """
    Convenience layer that perorms an affine transform followed by a ReLU

    Inputs:
    - x: Input to the affine layer
    - w, b: Weights for the affine layer

    Returns a tuple of:
    - out: Output from the ReLU
    - cache: Object to give to the backward pass
    """
    a, fc_cache = affine_forward(x, w, b)
    out, relu_cache = relu_forward(a)
    cache = (fc_cache, relu_cache)
    return out, cache


def affine_relu_backward(dout, cache):
    """
    Backward pass for the affine-relu convenience layer
    """
    fc_cache, relu_cache = cache
    da = relu_backward(dout, relu_cache)
    dx, dw, db = affine_backward(da, fc_cache)
    return dx, dw, db


def affine_batchnorm_relu_forward(x, w, b, gamma, beta, bn_params):
    """
    Convenience layer that perorms an affine transform followed by a ReLU
    Inputs:
    - x: Input to the affine layer
    - w, b: Weights for the affine layer
    Returns a tuple of:
    - out: Output from the ReLU
    - cache: Object to give to the backward pass
    """
    out, fc_cache = affine_forward(x, w, b)
    out, bn_cache = batchnorm_forward(out, gamma, beta, bn_params)
    out, relu_cache = relu_forward(out)
    cache = (fc_cache, bn_cache, relu_cache)
    return out, cache


def affine_batchnorm_relu_backward(dout, cache):
    """
    Backward pass for the affine-relu convenience layer
    """
    fc_cache, bn_cache, relu_cache = cache
    dx = relu_backward(dout, relu_cache)
    dx, dgamma, dbeta = batchnorm_backward(dx, bn_cache)
    dx, dw, db = affine_backward(dx, fc_cache)
    return dx, dw, db, dgamma, dbeta


# new layers
def lstm_forward(xs, w, u, b, h_n1=None, c_n1=None):
    """
    Computes the forward pass for a long short term memory layer.
    ref: https://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/

    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
    examples, where each example x[i] has shape (d_1, ..., d_k). We will
    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
    then transform it to an output vector of dimension M.

    Inputs:
    - xs: A numpy array containing input data, of shape (N, T, d_1, ..., d_k)
    - w: A numpy array of input weights, of shape (D, 4M)
    - u: A numpy array of state weights, of shape (M, 4M)
    - b: A numpy array of biases, of shape (4M,)
    - h_n1: A numpy array containing hidden state h_-1(see ref), of shape (N, M)
    - c_n1: A numpy array containing cell state c_-1, of shape (N, M)

    Returns a tuple of:
    - outs: A numpy array of hidden states, each generated by one time step, of shape (N, T, M)
    - caches: A list of T caches for back propagation, each generated by one time step
    """
    N, T = np.shape(xs)[0:2]
    outs = [None] * T
    #############################################################################
    # Implement the affine forward pass. Store the result in out.               #
    # Will need to reshape the input into rows.                                 #
    #############################################################################
    caches = [None] * T
    h_prev, c_prev = h_n1, c_n1
    for t in range(T):
        h, c, caches[t] = lstm_forward_unit(xs[:, t, ...], w, u, b, h_prev, c_prev)
        outs[t] = h
        # set up for next iteration (next time step)
        h_prev, c_prev = h, c
    outs = np.array(outs).transpose((1, 0, 2))
    #############################################################################
    #                                                                           #
    #############################################################################
    return outs, caches


def lstm_backward(douts, caches, dh_Tm1=None, dc_T=None, f_T=None):
    """
    Computes the backward pass for a LSTM layer.
    ref: https://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/

    Inputs:
    - dout: Upstream derivative, of shape (N, M)
    - caches: Caches from the forward pass
    - dh_Tm1: derivative of hidden state h_T-1(see ref), of shape (N, M)
    - dc_T: derivative of cell state c_T, of shape (N, M)
    - f_T: value of the forget gate f_T, of shape (N, M)

    Returns a tuple of:
    - dxs: Gradient with respect to xs, of shape (N, T, d1, ..., d_k)
    - dw: Gradient with respect to w, of shape (D, 4M)
    - du: Gradient with respect to u, of shape (M, 4M)
    - db: Gradient with respect to b, of shape (4M,)
    """
    _, _, _, _, _, w, u = caches[0]
    D = np.shape(w)[0]
    M = np.shape(u)[0]
    N, T = np.shape(douts)[0:2]
    dxs, dw, du, db = None, None, None, None
    #############################################################################
    # Implement the affine backward pass.                                       #
    #############################################################################
    dxs = [None] * T
    dw = np.zeros((D, 4*M))
    du = np.zeros((M, 4*M))
    db = np.zeros(4*M)
    dh, dc_next, f_next = dh_Tm1, dc_T, f_T
    for t in reversed(range(T)):
        dxs[t], dw_mid, du_mid, db_mid, dh_prev, dc, f = lstm_backward_unit(douts[:, t, :], caches[t],
                                                                           dh, dc_next, f_next)
        dw += dw_mid
        du += du_mid
        db += db_mid
        # set up for next iteration (previous time step)
        dh, dc_next, f_next = dh_prev, dc, f
    dxs = np.swapaxes(dxs, 0, 1)
    # Note: Gradient here is the sum of the gradients of N data
    #############################################################################
    #                                                                           #
    #############################################################################
    # update: dw, du, db
    # time cache: dh_prev, dc, f
    # space cache: dxs
    return dxs, dw, du, db


def batchnorm_lstm_forward_unit(x, w, u, b, gammas, betas, bn_params, h_prev=None, c_prev=None):
    """
    Computes the forward pass for a single time step of a long short term memory layer with batch normalization.
    ref: https://arxiv.org/pdf/1603.09025.pdf

    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
    examples, where each example x[i] has shape (d_1, ..., d_k). We will
    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
    then transform it to an output vector of dimension M.

    Inputs:
    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)
    - w: A numpy array of input weights, of shape (D, 4M)
    - u: A numpy array of state weights, of shape (M, 4M)
    - b: A numpy array of biases, of shape (4M,)
    - gammas: A tuple of gamma_x, gamma_h, gamma_c(see ref)
    - betas: A tuple of beta_x, beta_h, beta_c
    - bn_params: A list of bn_param_x, bn_param_h, bn_param_c
    - h_prev: A numpy array containing hidden state generated by previous step, of shape (N, M)
    - c_prev: A numpy array containing cell state generated by previous step, of shape (N, M)

    Returns a tuple of:
    - h: new hidden state, of shape (N, M)
    - c: new cell state, of shape (N, M)
    - caches: caches for back propagation,
      - cache1: Time cache to be passed to the previous step of propagation
      - caches2: A tuple of caches to for the current step of propagation
    """
    gamma_x, gamma_h, gamma_c = gammas
    beta_x, beta_h, beta_c = betas
    bn_param_x, bn_param_h, bn_param_c = bn_params

    N = np.shape(x)[0]
    M = np.shape(u)[0]
    h_prev = np.zeros((N, M)) if h_prev is None else h_prev
    c_prev = np.zeros((N, M)) if c_prev is None else c_prev
    h, c = None, None
    #############################################################################
    # Implement the affine forward pass. Store the result in out.               #
    # Will need to reshape the input into rows.                                 #
    #############################################################################
    x_fc, x_fc_cache = affine_forward(x, w, 0)
    h_fc, h_fc_cache = affine_forward(h_prev, u, 0)
    x_fc_bn, x_bn_cache = batchnorm_forward(x_fc, gamma_x, beta_x, bn_param_x)
    h_fc_bn, h_bn_cache = batchnorm_forward(h_fc, gamma_h, beta_h, bn_param_h)

    mid = x_fc_bn + h_fc_bn + b
    a, a_cache = tanh_forward(mid[:, :M])  # N*M
    i, i_cache = sigmoid_forward(mid[:, M:(2*M)])  # N*M
    f, f_cache = sigmoid_forward(mid[:, (2*M):(3*M)])  # N*M
    o, o_cache = sigmoid_forward(mid[:, (3*M):(4*M)])  # N*M

    c = a * i + f * c_prev  # N*M
    c_bn, c_bn_cache = batchnorm_forward(c, gamma_c, beta_c, bn_param_c)
    c_bn_tanh, c_bn_tanh_cache = tanh_forward(c_bn)
    h = c_bn_tanh * o  # N*M
    #############################################################################
    #                                                                           #
    #############################################################################
    cache1 = (h_prev, c_prev)
    caches2 = (x_fc_cache, h_fc_cache, x_bn_cache, h_bn_cache,
               a_cache, i_cache, f_cache, o_cache, c_bn_cache, c_bn_tanh_cache)
    caches = [cache1, caches2]
    return h, c, caches


def batchnorm_lstm_backward_unit(dout, caches, dh=None, dc_next=None, f_next=None):
    """
    Computes the backward pass for a BatchNorm-LSTM unit.
    ref: https://arxiv.org/pdf/1603.09025.pdf

    Inputs:
    - dout: Upstream derivative, of shape (N, M)
    - caches: Caches from the forward pass
    - dh: derivative of hidden state passed from the next step, of shape (N, M)
    - dc_next: derivative of cell state of the next step, of shape (N, M)
    - f_next: value of forget gate of the next step, of shape (N, M)

    Returns a tuple of:
    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
    - dw: Gradient with respect to w, of shape (D, 4M)
    - du: Gradient with respect to u, of shape (D, 4M)
    - db: Gradient with respect to b, of shape (4M,)
    - dgammas: Tuple of gradients with respect to the gammas
    - dbetas: Tuple of gradients with respect to the betas
    - dh_prev, dc, f: Time cache to be passed to the previous step of propagation. Correspond to dh, dc_next, f_next
    """
    cache1, caches2 = caches
    h_prev, c_prev = cache1
    x_fc_cache, h_fc_cache, x_bn_cache, h_bn_cache, \
    a, i, f, o, c_bn_cache, c_bn_tanh_cache = caches2

    N, M = np.shape(h_prev)
    dh = np.zeros((N, M)) if dh is None else dh
    dc_next = np.zeros((N, M)) if dc_next is None else dc_next
    f_next = np.zeros((N, M)) if f_next is None else f_next
    dx, dw, du, db, dgammas, dbetas = None, None, None, None, None, None
    #############################################################################
    # Implement the affine backward pass                                        #
    #############################################################################
    dy = dout + dh

    dc_bn_tanh = o * dy
    dc_bn = tanh_backward(dc_bn_tanh, c_bn_tanh_cache)

    dc, dgamma_c, dbeta_c = batchnorm_backward_alt(dc_bn, c_bn_cache)
    dc = dc + dc_next * f_next

    da = i * dc
    dmid_a = tanh_backward(da, a)
    di = a * dc
    dmid_i = sigmoid_backward(di, i)
    df = c_prev * dc
    dmid_f = sigmoid_backward(df, f)
    do = c_bn_tanh_cache * dy
    dmid_o = sigmoid_backward(do, o)
    dmid = np.concatenate((dmid_a, dmid_i, dmid_f, dmid_o), axis=1)  # N*4M

    dfc_h_prev, dgamma_h, dbeta_h = batchnorm_backward_alt(dmid, h_bn_cache)
    dfc_x, dgamma_x, dbeta_x = batchnorm_backward_alt(dmid, x_bn_cache)
    dh_prev, du, _ = affine_backward(dfc_h_prev, h_fc_cache)
    dx, dw, _ = affine_backward(dfc_x, x_fc_cache)

    N = np.shape(h_prev)[0]
    db = np.dot(np.ones(N), dmid)
    # Note: Gradient here is the sum of the gradients of N data
    #############################################################################
    #                                                                           #
    #############################################################################
    dgammas = (dgamma_x, dgamma_h, dgamma_c)
    dbetas = (dbeta_x, dbeta_h, dbeta_c)
    # update: dw, du, db, dgammas, dbetas
    # time intermediates: dh_prev, dc, f
    # space intermediates: dx
    return dx, dw, du, db, dgammas, dbetas, dh_prev, dc, f


def batchnorm_lstm_forward(xs, w, u, b, gammas, betas, bn_params, h_n1=None, c_n1=None):
    """
    Computes the forward pass for a long short term memory layer with batch normalization.
    ref: https://arxiv.org/pdf/1603.09025.pdf

    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N
    examples, where each example x[i] has shape (d_1, ..., d_k). We will
    reshape each input into a vector of dimension D = d_1 * ... * d_k, and
    then transform it to an output vector of dimension M.

    Inputs:
    - xs: A numpy array containing input data, of shape (N, T, d_1, ..., d_k)
    - w: A numpy array of input weights, of shape (D, 4M)
    - u: A numpy array of state weights, of shape (M, 4M)
    - b: A numpy array of biases, of shape (4M,)
    - gammas: A tuple of gamma_x, gamma_h, gamma_c(see ref)
    - betas: A tuple of beta_x, beta_h, beta_c
    - bn_params: A list of bn_param_x, bn_param_h, bn_param_c
    - h_n1: A numpy array containing hidden state h_-1(see ref), of shape (N, M)
    - c_n1: A numpy array containing cell state c_-1, of shape (N, M)

    Returns a tuple of:
    - outs: A numpy array of hidden states, each generated by one time step, of shape (N, T, M)
    - cacheses: A list of T caches for back propagation, each generated by one time step
    """
    N, T = np.shape(xs)[0:2]
    outs = [None] * T
    #############################################################################
    # Implement the affine forward pass. Store the result in out.               #
    # Will need to reshape the input into rows.                                 #
    #############################################################################
    cacheses = [None] * T
    h_prev, c_prev = h_n1, c_n1
    for t in range(T):
        h, c, cacheses[t] = batchnorm_lstm_forward_unit(xs[:, t, ...], w, u, b, gammas, betas, bn_params,
                                                        h_prev, c_prev)
        outs[t] = h
        # set up for next iteration (next time step)
        h_prev, c_prev = h, c
    outs = np.array(outs).transpose((1, 0, 2))
    #############################################################################
    #                                                                           #
    #############################################################################
    return outs, cacheses


def batchnorm_lstm_backward(douts, cacheses, dh_Tm1=None, dc_T=None, f_T=None):
    """
    Computes the backward pass for a BatchNorm-LSTM layer.
    ref: https://arxiv.org/pdf/1603.09025.pdf

    Inputs:
    - douts: Upstream derivative, of shape (N, T, M)
    - cacheses: Caches from the forward pass
    - dh_Tm1: derivative of hidden state h_T-1(see ref), of shape (N, M)
    - dc_T: derivative of cell state c_T, of shape (N, M)
    - f_T: value of the forget gate f_T, of shape (N, M)

    Returns a tuple of:
    - dxs: Gradient with respect to xs, of shape (N, T, d1, ..., d_k)
    - dw: Gradient with respect to w, of shape (D, 4M)
    - du: Gradient with respect to u, of shape (D, 4M)
    - db: Gradient with respect to b, of shape (4M,)
    - dgammas: Tuple of gradients with respect to the gammas
    - dbetas: Tuple of gradients with respect to the betas
    """
    _, w, _ = cacheses[0][1][0]
    _, u, _ = cacheses[0][1][1]
    D = np.shape(w)[0]
    M = np.shape(u)[0]
    N, T = np.shape(douts)[0:2]
    dxs, dw, du, db = None, None, None, None
    #############################################################################
    # Implement the affine backward pass.                                       #
    #############################################################################
    dxs = [None] * T
    dw = np.zeros((D, 4*M))
    du = np.zeros((M, 4*M))
    db = np.zeros(4 * M)
    dgamma_x = np.zeros(4 * M)
    dbeta_x = np.zeros(4 * M)
    dgamma_h = np.zeros(4 * M)
    dbeta_h = np.zeros(4 * M)
    dgamma_c = np.zeros(M)
    dbeta_c = np.zeros(M)
    dh, dc_next, f_next = dh_Tm1, dc_T, f_T
    for t in reversed(range(T)):
        dxs[t], dw_mid, du_mid, db_mid, dgammas_mid, dbetas_mid, dh_prev, dc, f = \
            batchnorm_lstm_backward_unit(douts[:, t, :], cacheses[t], dh, dc_next, f_next)
        dw += dw_mid
        du += du_mid
        db += db_mid
        dgamma_x_mid, dgamma_h_mid, dgamma_c_mid = dgammas_mid
        dbeta_x_mid, dbeta_h_mid, dbeta_c_mid = dbetas_mid
        dgamma_x += dgamma_x_mid
        dbeta_x += dbeta_x_mid
        dgamma_h += dgamma_h_mid
        dbeta_h += dbeta_h_mid
        dgamma_c += dgamma_c_mid
        dbeta_c += dbeta_c_mid
        # set up for next iteration (previous time step)
        dh, dc_next, f_next = dh_prev, dc, f
    dxs = np.swapaxes(dxs, 0, 1)
    # Note: Gradient here is the sum of the gradients of N data
    #############################################################################
    #                                                                           #
    #############################################################################
    dgammas = (dgamma_x, dgamma_h, dgamma_c)
    dbetas = (dbeta_x, dbeta_h, dbeta_c)
    # update: dw, du, db, dgammas, dbetas
    # time cache: dh_prev, dc, f
    # space cache: dxs
    return dxs, dw, du, db, dgammas, dbetas

